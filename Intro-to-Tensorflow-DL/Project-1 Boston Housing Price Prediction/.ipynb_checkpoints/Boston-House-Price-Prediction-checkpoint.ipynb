{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston House Price Prediction using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressor using TF Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DESCR': \"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\", 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
      "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
      "        4.9800e+00],\n",
      "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
      "        9.1400e+00],\n",
      "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
      "        4.0300e+00],\n",
      "       ...,\n",
      "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
      "        5.6400e+00],\n",
      "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
      "        6.4800e+00],\n",
      "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
      "        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
      "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
      "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
      "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
      "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
      "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
      "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
      "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
      "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
      "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
      "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
      "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
      "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
      "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
      "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
      "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
      "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
      "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
      "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
      "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
      "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
      "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
      "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
      "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
      "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
      "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
      "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
      "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
      "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
      "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
      "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
      "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
      "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
      "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
      "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
      "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
      "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
      "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
      "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
      "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
      "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
      "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
      "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
      "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
      "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
      "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])}\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Print out the Dataset\n",
    "print(boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate Data into Features and Labels\n",
    "# Features\n",
    "features_df = pd.DataFrame(np.array(boston.data), columns=[boston.feature_names])\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the Dataset\n",
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.199458</td>\n",
       "      <td>0.404471</td>\n",
       "      <td>-0.055295</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>-0.219940</td>\n",
       "      <td>0.350784</td>\n",
       "      <td>-0.377904</td>\n",
       "      <td>0.622029</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>0.288250</td>\n",
       "      <td>-0.377365</td>\n",
       "      <td>0.452220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.199458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.404471</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>-0.055295</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.417521</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.219940</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.350784</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.377904</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.622029</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.579564</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.288250</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.377365</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.452220</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.199458  0.404471 -0.055295  0.417521 -0.219940  0.350784   \n",
       "ZN      -0.199458  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "INDUS    0.404471 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "CHAS    -0.055295 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "NOX      0.417521 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "RM      -0.219940  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "AGE      0.350784 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "DIS     -0.377904  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "RAD      0.622029 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "TAX      0.579564 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "PTRATIO  0.288250 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "B       -0.377365  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "LSTAT    0.452220 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "CRIM    -0.377904  0.622029  0.579564  0.288250 -0.377365  0.452220  \n",
       "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
       "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
       "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
       "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
       "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
       "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
       "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
       "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
       "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
       "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
       "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
       "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Feature Correlation to find Important Features\n",
    "features_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels\n",
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "labels_df = pd.DataFrame(np.array(boston.target), columns=['labels'])\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Preprocessing Method and Fit Training Data to it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X_train to be the Scaled Version of Data\n",
    "# This process scales all the values in all 6 columns and replaces them with the new values\n",
    "X_train = pd.DataFrame(data=scaler.transform(X_train), columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.411893</td>\n",
       "      <td>1.844475</td>\n",
       "      <td>-0.854602</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.294726</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>-0.799422</td>\n",
       "      <td>1.674506</td>\n",
       "      <td>-0.403534</td>\n",
       "      <td>-0.673066</td>\n",
       "      <td>-0.853362</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.406165</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.054399</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.224165</td>\n",
       "      <td>-0.311287</td>\n",
       "      <td>-2.138503</td>\n",
       "      <td>0.699753</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.602036</td>\n",
       "      <td>0.357624</td>\n",
       "      <td>0.368614</td>\n",
       "      <td>-1.040342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.088066</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.174563</td>\n",
       "      <td>-0.067526</td>\n",
       "      <td>0.798264</td>\n",
       "      <td>-0.350920</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.416805</td>\n",
       "      <td>0.034870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.408848</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-1.039712</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.368607</td>\n",
       "      <td>-0.378173</td>\n",
       "      <td>-0.742741</td>\n",
       "      <td>-0.132142</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.655308</td>\n",
       "      <td>-0.853362</td>\n",
       "      <td>0.393909</td>\n",
       "      <td>-0.368883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-0.396653</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.554343</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.518550</td>\n",
       "      <td>0.225285</td>\n",
       "      <td>-0.562072</td>\n",
       "      <td>0.341093</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.708581</td>\n",
       "      <td>0.543929</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.794579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.415999</td>\n",
       "      <td>3.517055</td>\n",
       "      <td>-1.452203</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.312367</td>\n",
       "      <td>1.448551</td>\n",
       "      <td>-1.639004</td>\n",
       "      <td>2.336891</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-1.069647</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>0.423347</td>\n",
       "      <td>-1.144206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-0.378154</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.726336</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.421528</td>\n",
       "      <td>2.960169</td>\n",
       "      <td>0.358989</td>\n",
       "      <td>-0.449313</td>\n",
       "      <td>-0.172453</td>\n",
       "      <td>-0.590198</td>\n",
       "      <td>-0.480751</td>\n",
       "      <td>0.304721</td>\n",
       "      <td>-1.242219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.415848</td>\n",
       "      <td>3.963076</td>\n",
       "      <td>-1.437627</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.241805</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>-0.980091</td>\n",
       "      <td>2.158354</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.892073</td>\n",
       "      <td>-1.552008</td>\n",
       "      <td>0.390311</td>\n",
       "      <td>-1.270013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.384871</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.187037</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.068721</td>\n",
       "      <td>-0.373714</td>\n",
       "      <td>0.787637</td>\n",
       "      <td>-0.478317</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.607955</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>-0.133360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-0.376133</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.726336</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.395068</td>\n",
       "      <td>2.931928</td>\n",
       "      <td>0.079128</td>\n",
       "      <td>-0.085563</td>\n",
       "      <td>-0.172453</td>\n",
       "      <td>-0.590198</td>\n",
       "      <td>-0.480751</td>\n",
       "      <td>0.238213</td>\n",
       "      <td>-1.270013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.397685</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.622849</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.915459</td>\n",
       "      <td>-0.094280</td>\n",
       "      <td>-2.184556</td>\n",
       "      <td>0.907966</td>\n",
       "      <td>-0.750156</td>\n",
       "      <td>-1.028213</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>0.407320</td>\n",
       "      <td>-0.759470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.405439</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.761317</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.465629</td>\n",
       "      <td>-0.644230</td>\n",
       "      <td>-0.239700</td>\n",
       "      <td>-0.217137</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.755934</td>\n",
       "      <td>0.357624</td>\n",
       "      <td>0.223057</td>\n",
       "      <td>-0.178710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-0.416012</td>\n",
       "      <td>1.063938</td>\n",
       "      <td>-1.408476</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.968380</td>\n",
       "      <td>1.436660</td>\n",
       "      <td>-0.668348</td>\n",
       "      <td>1.540387</td>\n",
       "      <td>-0.981238</td>\n",
       "      <td>-0.726338</td>\n",
       "      <td>-1.365702</td>\n",
       "      <td>0.410373</td>\n",
       "      <td>-1.044731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.412717</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-1.132996</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.553831</td>\n",
       "      <td>0.194072</td>\n",
       "      <td>0.203117</td>\n",
       "      <td>-0.354378</td>\n",
       "      <td>-0.865697</td>\n",
       "      <td>-0.809206</td>\n",
       "      <td>-0.294446</td>\n",
       "      <td>0.397398</td>\n",
       "      <td>-0.648292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.409544</td>\n",
       "      <td>2.625012</td>\n",
       "      <td>-1.303531</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.338827</td>\n",
       "      <td>0.104891</td>\n",
       "      <td>-1.702770</td>\n",
       "      <td>1.919695</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.288323</td>\n",
       "      <td>-1.691737</td>\n",
       "      <td>0.121440</td>\n",
       "      <td>-1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.415839</td>\n",
       "      <td>2.178991</td>\n",
       "      <td>-1.202959</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.330007</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>-1.748823</td>\n",
       "      <td>1.147441</td>\n",
       "      <td>-0.981238</td>\n",
       "      <td>-0.838801</td>\n",
       "      <td>-1.319126</td>\n",
       "      <td>0.213681</td>\n",
       "      <td>-1.207110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-0.129575</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.223885</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.469310</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.989562</td>\n",
       "      <td>-0.830782</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.021962</td>\n",
       "      <td>-1.738313</td>\n",
       "      <td>-0.654315</td>\n",
       "      <td>-0.224059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.413327</td>\n",
       "      <td>3.182539</td>\n",
       "      <td>-1.334140</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.206525</td>\n",
       "      <td>-0.167111</td>\n",
       "      <td>-1.054485</td>\n",
       "      <td>1.171643</td>\n",
       "      <td>-0.865697</td>\n",
       "      <td>-0.347514</td>\n",
       "      <td>-1.738313</td>\n",
       "      <td>0.399797</td>\n",
       "      <td>-0.760933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.423581</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.421890</td>\n",
       "      <td>0.705376</td>\n",
       "      <td>0.865573</td>\n",
       "      <td>-0.723410</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>-3.890029</td>\n",
       "      <td>0.703403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-0.385889</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.187037</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.068721</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>0.337734</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.607955</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>0.428690</td>\n",
       "      <td>-0.165544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.413198</td>\n",
       "      <td>3.071033</td>\n",
       "      <td>-1.099472</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.409389</td>\n",
       "      <td>-0.592207</td>\n",
       "      <td>-1.738195</td>\n",
       "      <td>2.588371</td>\n",
       "      <td>-0.981238</td>\n",
       "      <td>-0.542845</td>\n",
       "      <td>-0.946515</td>\n",
       "      <td>0.415170</td>\n",
       "      <td>-0.494690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.410573</td>\n",
       "      <td>1.286948</td>\n",
       "      <td>-0.695727</td>\n",
       "      <td>3.736705</td>\n",
       "      <td>-0.924279</td>\n",
       "      <td>0.718753</td>\n",
       "      <td>-1.249325</td>\n",
       "      <td>0.118857</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.903912</td>\n",
       "      <td>-0.387598</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-1.331454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.415068</td>\n",
       "      <td>3.294044</td>\n",
       "      <td>-1.522166</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.250626</td>\n",
       "      <td>0.161372</td>\n",
       "      <td>-1.150133</td>\n",
       "      <td>2.572668</td>\n",
       "      <td>-0.865697</td>\n",
       "      <td>-0.554683</td>\n",
       "      <td>-0.527328</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-1.003770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.311667</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.174563</td>\n",
       "      <td>-0.048203</td>\n",
       "      <td>-0.122796</td>\n",
       "      <td>-0.194904</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.276722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>4.298544</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.122004</td>\n",
       "      <td>-0.107657</td>\n",
       "      <td>0.373159</td>\n",
       "      <td>-0.944637</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>-3.688322</td>\n",
       "      <td>0.276244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1.377286</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.307228</td>\n",
       "      <td>-2.072612</td>\n",
       "      <td>1.127721</td>\n",
       "      <td>-1.075923</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.172467</td>\n",
       "      <td>2.631471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.111257</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.223885</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>2.815479</td>\n",
       "      <td>-1.489963</td>\n",
       "      <td>0.915168</td>\n",
       "      <td>-1.005382</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.021962</td>\n",
       "      <td>-1.738313</td>\n",
       "      <td>-2.927176</td>\n",
       "      <td>0.513230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.411404</td>\n",
       "      <td>0.439508</td>\n",
       "      <td>-0.807960</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.994840</td>\n",
       "      <td>-0.410873</td>\n",
       "      <td>-1.656717</td>\n",
       "      <td>1.433207</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.969022</td>\n",
       "      <td>-0.760209</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.614646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.318662</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.187037</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.068721</td>\n",
       "      <td>-0.226565</td>\n",
       "      <td>-0.544359</td>\n",
       "      <td>-0.571331</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.607955</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.973050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-0.407160</td>\n",
       "      <td>1.509959</td>\n",
       "      <td>-1.128624</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.012481</td>\n",
       "      <td>1.353425</td>\n",
       "      <td>-1.036772</td>\n",
       "      <td>0.353722</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.051558</td>\n",
       "      <td>-1.505431</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-1.059360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.863779</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.548692</td>\n",
       "      <td>-0.132925</td>\n",
       "      <td>1.010817</td>\n",
       "      <td>-0.796928</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.246390</td>\n",
       "      <td>0.789713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.170761</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.398749</td>\n",
       "      <td>-1.663866</td>\n",
       "      <td>1.127721</td>\n",
       "      <td>-1.075971</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>-1.593291</td>\n",
       "      <td>1.089602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.738467</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.698635</td>\n",
       "      <td>-0.086848</td>\n",
       "      <td>1.127721</td>\n",
       "      <td>-1.277798</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.098653</td>\n",
       "      <td>-0.453730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-0.377239</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.560582</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.636894</td>\n",
       "      <td>-0.494108</td>\n",
       "      <td>0.897456</td>\n",
       "      <td>-0.894696</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>0.179288</td>\n",
       "      <td>1.289151</td>\n",
       "      <td>0.230580</td>\n",
       "      <td>0.624408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.405013</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.171004</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.042260</td>\n",
       "      <td>0.654840</td>\n",
       "      <td>0.475893</td>\n",
       "      <td>-0.553468</td>\n",
       "      <td>-0.403534</td>\n",
       "      <td>0.149692</td>\n",
       "      <td>-0.294446</td>\n",
       "      <td>0.419640</td>\n",
       "      <td>-0.361569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-0.410757</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.873550</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.324506</td>\n",
       "      <td>-0.430195</td>\n",
       "      <td>-0.806507</td>\n",
       "      <td>0.471611</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-1.081485</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.423010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.390120</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.622849</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.915459</td>\n",
       "      <td>-0.999467</td>\n",
       "      <td>-0.218445</td>\n",
       "      <td>1.084103</td>\n",
       "      <td>-0.750156</td>\n",
       "      <td>-1.028213</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>0.522007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.387697</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.217646</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.292906</td>\n",
       "      <td>-0.379659</td>\n",
       "      <td>-0.101541</td>\n",
       "      <td>-0.682353</td>\n",
       "      <td>-0.403534</td>\n",
       "      <td>-0.092992</td>\n",
       "      <td>0.357624</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>0.042184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1.463654</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.548692</td>\n",
       "      <td>-1.442400</td>\n",
       "      <td>1.032072</td>\n",
       "      <td>-0.830446</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>-0.083102</td>\n",
       "      <td>1.796170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.317682</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.245486</td>\n",
       "      <td>0.194072</td>\n",
       "      <td>0.986019</td>\n",
       "      <td>-1.034002</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>0.985738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.413903</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-1.312277</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.827257</td>\n",
       "      <td>0.231230</td>\n",
       "      <td>-0.335349</td>\n",
       "      <td>1.071858</td>\n",
       "      <td>-0.750156</td>\n",
       "      <td>-1.093324</td>\n",
       "      <td>0.124742</td>\n",
       "      <td>0.403613</td>\n",
       "      <td>-1.085691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.404125</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.382351</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.280405</td>\n",
       "      <td>-0.118062</td>\n",
       "      <td>-0.487678</td>\n",
       "      <td>-0.505304</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.134426</td>\n",
       "      <td>1.149422</td>\n",
       "      <td>0.396744</td>\n",
       "      <td>0.053887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-0.406791</td>\n",
       "      <td>1.509959</td>\n",
       "      <td>-1.128624</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.012481</td>\n",
       "      <td>1.343020</td>\n",
       "      <td>-1.483132</td>\n",
       "      <td>1.272389</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.051558</td>\n",
       "      <td>-1.505431</td>\n",
       "      <td>0.364034</td>\n",
       "      <td>-1.428004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>-0.372799</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.726336</td>\n",
       "      <td>3.736705</td>\n",
       "      <td>-0.395068</td>\n",
       "      <td>1.005619</td>\n",
       "      <td>0.720328</td>\n",
       "      <td>-0.465015</td>\n",
       "      <td>-0.172453</td>\n",
       "      <td>-0.590198</td>\n",
       "      <td>-0.480751</td>\n",
       "      <td>0.377227</td>\n",
       "      <td>-0.427398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.377786</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.187037</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.068721</td>\n",
       "      <td>-0.535726</td>\n",
       "      <td>0.532574</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>-0.607955</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>0.366324</td>\n",
       "      <td>0.833599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.393682</td>\n",
       "      <td>0.484110</td>\n",
       "      <td>-0.775893</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.065402</td>\n",
       "      <td>-0.995008</td>\n",
       "      <td>0.072043</td>\n",
       "      <td>1.980728</td>\n",
       "      <td>-0.287994</td>\n",
       "      <td>-0.454058</td>\n",
       "      <td>0.311047</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.852616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.683346</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.284086</td>\n",
       "      <td>-1.054462</td>\n",
       "      <td>0.086214</td>\n",
       "      <td>-0.848309</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>-3.853722</td>\n",
       "      <td>0.662443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.409798</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>0.108848</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.187064</td>\n",
       "      <td>0.473505</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>-0.648979</td>\n",
       "      <td>-0.981238</td>\n",
       "      <td>-0.791448</td>\n",
       "      <td>1.195998</td>\n",
       "      <td>0.380389</td>\n",
       "      <td>-0.433250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-0.343752</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.560582</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.636894</td>\n",
       "      <td>0.145022</td>\n",
       "      <td>1.053327</td>\n",
       "      <td>-0.721585</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>0.179288</td>\n",
       "      <td>1.289151</td>\n",
       "      <td>0.312463</td>\n",
       "      <td>-0.221133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.413435</td>\n",
       "      <td>2.848023</td>\n",
       "      <td>-1.200044</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.091862</td>\n",
       "      <td>1.114122</td>\n",
       "      <td>-1.855099</td>\n",
       "      <td>0.754399</td>\n",
       "      <td>-0.750156</td>\n",
       "      <td>-0.915750</td>\n",
       "      <td>-0.061564</td>\n",
       "      <td>0.419967</td>\n",
       "      <td>-1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.044826</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.421890</td>\n",
       "      <td>0.150968</td>\n",
       "      <td>0.716786</td>\n",
       "      <td>-0.606481</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.374283</td>\n",
       "      <td>0.295262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.664893</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>1.008165</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>1.245486</td>\n",
       "      <td>-0.121035</td>\n",
       "      <td>0.865573</td>\n",
       "      <td>-0.979067</td>\n",
       "      <td>1.676198</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>0.371331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.395227</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.217646</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>0.292906</td>\n",
       "      <td>-0.898396</td>\n",
       "      <td>-1.394569</td>\n",
       "      <td>-0.495316</td>\n",
       "      <td>-0.403534</td>\n",
       "      <td>-0.092992</td>\n",
       "      <td>0.357624</td>\n",
       "      <td>0.394563</td>\n",
       "      <td>0.726809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.405730</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>0.240029</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.012481</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>-0.820677</td>\n",
       "      <td>0.322941</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.051558</td>\n",
       "      <td>0.124742</td>\n",
       "      <td>0.284878</td>\n",
       "      <td>-0.540039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.408686</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.975579</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.906638</td>\n",
       "      <td>-0.228052</td>\n",
       "      <td>-0.402657</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.750156</td>\n",
       "      <td>-0.945345</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.414843</td>\n",
       "      <td>-0.613183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.401800</td>\n",
       "      <td>0.617916</td>\n",
       "      <td>-0.882295</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.871358</td>\n",
       "      <td>0.724699</td>\n",
       "      <td>-0.877358</td>\n",
       "      <td>1.993214</td>\n",
       "      <td>-0.172453</td>\n",
       "      <td>-0.726338</td>\n",
       "      <td>0.590505</td>\n",
       "      <td>0.419531</td>\n",
       "      <td>-0.458118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>-0.379639</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.554343</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.518550</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0.761506</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.708581</td>\n",
       "      <td>0.543929</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>-0.948181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-0.413833</td>\n",
       "      <td>-0.497137</td>\n",
       "      <td>-0.873550</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.324506</td>\n",
       "      <td>-0.563967</td>\n",
       "      <td>-0.303466</td>\n",
       "      <td>0.857114</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-1.081485</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.411136</td>\n",
       "      <td>-0.303054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.402931</td>\n",
       "      <td>0.060390</td>\n",
       "      <td>-0.482923</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.245125</td>\n",
       "      <td>-0.394523</td>\n",
       "      <td>0.521946</td>\n",
       "      <td>1.150851</td>\n",
       "      <td>-0.519075</td>\n",
       "      <td>-0.566522</td>\n",
       "      <td>-1.505431</td>\n",
       "      <td>0.433923</td>\n",
       "      <td>0.093385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.407686</td>\n",
       "      <td>2.178991</td>\n",
       "      <td>-1.383697</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-1.241805</td>\n",
       "      <td>0.452697</td>\n",
       "      <td>-1.143048</td>\n",
       "      <td>3.303866</td>\n",
       "      <td>-0.634616</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>-0.061564</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>-1.044731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "288 -0.411893  1.844475 -0.854602 -0.267615 -1.294726  0.060300 -0.799422   \n",
       "72  -0.406165 -0.497137 -0.054399 -0.267615 -1.224165 -0.311287 -2.138503   \n",
       "471  0.088066 -0.497137  1.008165 -0.267615 -0.174563 -0.067526  0.798264   \n",
       "176 -0.408848 -0.497137 -1.039712 -0.267615 -0.368607 -0.378173 -0.742741   \n",
       "320 -0.396653 -0.497137 -0.554343 -0.267615 -0.518550  0.225285 -0.562072   \n",
       "55  -0.415999  3.517055 -1.452203 -0.267615 -1.312367  1.448551 -1.639004   \n",
       "224 -0.378154 -0.497137 -0.726336 -0.267615 -0.421528  2.960169  0.358989   \n",
       "57  -0.415848  3.963076 -1.437627 -0.267615 -1.241805  0.804961 -0.980091   \n",
       "312 -0.384871 -0.497137 -0.187037 -0.267615 -0.068721 -0.373714  0.787637   \n",
       "233 -0.376133 -0.497137 -0.726336 -0.267615 -0.395068  2.931928  0.079128   \n",
       "43  -0.397685 -0.497137 -0.622849 -0.267615 -0.915459 -0.094280 -2.184556   \n",
       "36  -0.405439 -0.497137 -0.761317 -0.267615 -0.465629 -0.644230 -0.239700   \n",
       "341 -0.416012  1.063938 -1.408476 -0.267615 -0.968380  1.436660 -0.668348   \n",
       "91  -0.412717 -0.497137 -1.132996 -0.267615 -0.553831  0.194072  0.203117   \n",
       "298 -0.409544  2.625012 -1.303531 -0.267615 -1.338827  0.104891 -1.702770   \n",
       "194 -0.415839  2.178991 -1.202959 -0.267615 -1.330007  0.489855 -1.748823   \n",
       "168 -0.129575 -0.497137  1.223885 -0.267615  0.469310  0.066246  0.989562   \n",
       "201 -0.413327  3.182539 -1.334140 -0.267615 -1.206525 -0.167111 -1.054485   \n",
       "450  0.423581 -0.497137  1.008165 -0.267615  1.421890  0.705376  0.865573   \n",
       "315 -0.385889 -0.497137 -0.187037 -0.267615 -0.068721 -0.846373  0.337734   \n",
       "255 -0.413198  3.071033 -1.099472 -0.267615 -1.409389 -0.592207 -1.738195   \n",
       "274 -0.410573  1.286948 -0.695727  3.736705 -0.924279  0.718753 -1.249325   \n",
       "56  -0.415068  3.294044 -1.522166 -0.267615 -1.250626  0.161372 -1.150133   \n",
       "480  0.311667 -0.497137  1.008165 -0.267615 -0.174563 -0.048203 -0.122796   \n",
       "427  4.298544 -0.497137  1.008165 -0.267615  1.122004 -0.107657  0.373159   \n",
       "388  1.377286 -0.497137  1.008165 -0.267615  1.307228 -2.072612  1.127721   \n",
       "156 -0.111257 -0.497137  1.223885 -0.267615  2.815479 -1.489963  0.915168   \n",
       "53  -0.411404  0.439508 -0.807960 -0.267615 -0.994840 -0.410873 -1.656717   \n",
       "311 -0.318662 -0.497137 -0.187037 -0.267615 -0.068721 -0.226565 -0.544359   \n",
       "189 -0.407160  1.509959 -1.128624 -0.267615 -1.012481  1.353425 -1.036772   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "478  0.863779 -0.497137  1.008165 -0.267615  0.548692 -0.132925  1.010817   \n",
       "413  3.170761 -0.497137  1.008165 -0.267615  0.398749 -1.663866  1.127721   \n",
       "371  0.738467 -0.497137  1.008165 -0.267615  0.698635 -0.086848  1.127721   \n",
       "136 -0.377239 -0.497137  1.560582 -0.267615  0.636894 -0.494108  0.897456   \n",
       "111 -0.405013 -0.497137 -0.171004 -0.267615 -0.042260  0.654840  0.475893   \n",
       "339 -0.410757 -0.497137 -0.873550 -0.267615 -0.324506 -0.430195 -0.806507   \n",
       "49  -0.390120 -0.497137 -0.622849 -0.267615 -0.915459 -0.999467 -0.218445   \n",
       "498 -0.387697 -0.497137 -0.217646 -0.267615  0.292906 -0.379659 -0.101541   \n",
       "477  1.463654 -0.497137  1.008165 -0.267615  0.548692 -1.442400  1.032072   \n",
       "396  0.317682 -0.497137  1.008165 -0.267615  1.245486  0.194072  0.986019   \n",
       "5   -0.413903 -0.497137 -1.312277 -0.267615 -0.827257  0.231230 -0.335349   \n",
       "110 -0.404125 -0.497137 -0.382351 -0.267615 -0.280405 -0.118062 -0.487678   \n",
       "192 -0.406791  1.509959 -1.128624 -0.267615 -1.012481  1.343020 -1.483132   \n",
       "220 -0.372799 -0.497137 -0.726336  3.736705 -0.395068  1.005619  0.720328   \n",
       "316 -0.377786 -0.497137 -0.187037 -0.267615 -0.068721 -0.535726  0.532574   \n",
       "245 -0.393682  0.484110 -0.775893 -0.267615 -1.065402 -0.995008  0.072043   \n",
       "424  0.683346 -0.497137  1.008165 -0.267615  0.284086 -1.054462  0.086214   \n",
       "501 -0.409798 -0.497137  0.108848 -0.267615  0.187064  0.473505  0.033075   \n",
       "132 -0.343752 -0.497137  1.560582 -0.267615  0.636894  0.145022  1.053327   \n",
       "40  -0.413435  2.848023 -1.200044 -0.267615 -1.091862  1.114122 -1.855099   \n",
       "461  0.044826 -0.497137  1.008165 -0.267615  1.421890  0.150968  0.716786   \n",
       "393  0.664893 -0.497137  1.008165 -0.267615  1.245486 -0.121035  0.865573   \n",
       "495 -0.395227 -0.497137 -0.217646 -0.267615  0.292906 -0.898396 -1.394569   \n",
       "75  -0.405730 -0.497137  0.240029 -0.267615 -1.012481  0.017196 -0.820677   \n",
       "87  -0.408686 -0.497137 -0.975579 -0.267615 -0.906638 -0.228052 -0.402657   \n",
       "63  -0.401800  0.617916 -0.882295 -0.267615 -0.871358  0.724699 -0.877358   \n",
       "326 -0.379639 -0.497137 -0.554343 -0.267615 -0.518550  0.055841 -1.391026   \n",
       "337 -0.413833 -0.497137 -0.873550 -0.267615 -0.324506 -0.563967 -0.303466   \n",
       "11  -0.402931  0.060390 -0.482923 -0.267615 -0.245125 -0.394523  0.521946   \n",
       "351 -0.407686  2.178991 -1.383697 -0.267615 -1.241805  0.452697 -1.143048   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "288  1.674506 -0.403534 -0.673066 -0.853362  0.433923 -0.736065  \n",
       "72   0.699753 -0.634616 -0.602036  0.357624  0.368614 -1.040342  \n",
       "471 -0.350920  1.676198  1.534767  0.823387  0.416805  0.034870  \n",
       "176 -0.132142 -0.519075 -0.655308 -0.853362  0.393909 -0.368883  \n",
       "320  0.341093 -0.519075 -0.708581  0.543929  0.433923 -0.794579  \n",
       "55   2.336891 -0.519075 -1.069647 -0.247869  0.423347 -1.144206  \n",
       "224 -0.449313 -0.172453 -0.590198 -0.480751  0.304721 -1.242219  \n",
       "57   2.158354 -0.519075 -0.892073 -1.552008  0.390311 -1.270013  \n",
       "312 -0.478317 -0.634616 -0.607955 -0.014987  0.427381 -0.133360  \n",
       "233 -0.085563 -0.172453 -0.590198 -0.480751  0.238213 -1.270013  \n",
       "43   0.907966 -0.750156 -1.028213 -0.247869  0.407320 -0.759470  \n",
       "36  -0.217137 -0.519075 -0.755934  0.357624  0.223057 -0.178710  \n",
       "341  1.540387 -0.981238 -0.726338 -1.365702  0.410373 -1.044731  \n",
       "91  -0.354378 -0.865697 -0.809206 -0.294446  0.397398 -0.648292  \n",
       "298  1.919695 -0.519075 -0.288323 -1.691737  0.121440 -1.120800  \n",
       "194  1.147441 -0.981238 -0.838801 -1.319126  0.213681 -1.207110  \n",
       "168 -0.830782 -0.519075 -0.021962 -1.738313 -0.654315 -0.224059  \n",
       "201  1.171643 -0.865697 -0.347514 -1.738313  0.399797 -0.760933  \n",
       "450 -0.723410  1.676198  1.534767  0.823387 -3.890029  0.703403  \n",
       "315  0.055183 -0.634616 -0.607955 -0.014987  0.428690 -0.165544  \n",
       "255  2.588371 -0.981238 -0.542845 -0.946515  0.415170 -0.494690  \n",
       "274  0.118857 -0.634616 -0.903912 -0.387598  0.433923 -1.331454  \n",
       "56   2.572668 -0.865697 -0.554683 -0.527328  0.433923 -1.003770  \n",
       "480 -0.194904  1.676198  1.534767  0.823387  0.433923 -0.276722  \n",
       "427 -0.944637  1.676198  1.534767  0.823387 -3.688322  0.276244  \n",
       "388 -1.075923  1.676198  1.534767  0.823387  0.172467  2.631471  \n",
       "156 -1.005382 -0.519075 -0.021962 -1.738313 -2.927176  0.513230  \n",
       "53   1.433207 -0.634616 -0.969022 -0.760209  0.433923 -0.614646  \n",
       "311 -0.571331 -0.634616 -0.607955 -0.014987  0.433923 -0.973050  \n",
       "189  0.353722 -0.519075 -0.051558 -1.505431  0.433923 -1.059360  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "478 -0.796928  1.676198  1.534767  0.823387  0.246390  0.789713  \n",
       "413 -1.075971  1.676198  1.534767  0.823387 -1.593291  1.089602  \n",
       "371 -1.277798  1.676198  1.534767  0.823387  0.098653 -0.453730  \n",
       "136 -0.894696 -0.634616  0.179288  1.289151  0.230580  0.624408  \n",
       "111 -0.553468 -0.403534  0.149692 -0.294446  0.419640 -0.361569  \n",
       "339  0.471611 -0.519075 -1.081485  0.823387  0.433923 -0.423010  \n",
       "49   1.084103 -0.750156 -1.028213 -0.247869  0.433923  0.522007  \n",
       "498 -0.682353 -0.403534 -0.092992  0.357624  0.433923  0.042184  \n",
       "477 -0.830446  1.676198  1.534767  0.823387 -0.083102  1.796170  \n",
       "396 -1.034002  1.676198  1.534767  0.823387  0.433923  0.985738  \n",
       "5    1.071858 -0.750156 -1.093324  0.124742  0.403613 -1.085691  \n",
       "110 -0.505304 -0.519075 -0.134426  1.149422  0.396744  0.053887  \n",
       "192  1.272389 -0.519075 -0.051558 -1.505431  0.364034 -1.428004  \n",
       "220 -0.465015 -0.172453 -0.590198 -0.480751  0.377227 -0.427398  \n",
       "316  0.080922 -0.634616 -0.607955 -0.014987  0.366324  0.833599  \n",
       "245  1.980728 -0.287994 -0.454058  0.311047  0.349206  0.852616  \n",
       "424 -0.848309  1.676198  1.534767  0.823387 -3.853722  0.662443  \n",
       "501 -0.648979 -0.981238 -0.791448  1.195998  0.380389 -0.433250  \n",
       "132 -0.721585 -0.634616  0.179288  1.289151  0.312463 -0.221133  \n",
       "40   0.754399 -0.750156 -0.915750 -0.061564  0.419967 -1.558200  \n",
       "461 -0.606481  1.676198  1.534767  0.823387  0.374283  0.295262  \n",
       "393 -0.979067  1.676198  1.534767  0.823387  0.433923  0.371331  \n",
       "495 -0.495316 -0.403534 -0.092992  0.357624  0.394563  0.726809  \n",
       "75   0.322941 -0.519075 -0.051558  0.124742  0.284878 -0.540039  \n",
       "87  -0.039608 -0.750156 -0.945345  0.031589  0.414843 -0.613183  \n",
       "63   1.993214 -0.172453 -0.726338  0.590505  0.419531 -0.458118  \n",
       "326  0.761506 -0.519075 -0.708581  0.543929  0.433923 -0.948181  \n",
       "337  0.857114 -0.519075 -1.081485  0.823387  0.411136 -0.303054  \n",
       "11   1.150851 -0.519075 -0.566522 -1.505431  0.433923  0.093385  \n",
       "351  3.303866 -0.634616  0.025391 -0.061564  0.149134 -1.044731  \n",
       "\n",
       "[404 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want this dataframe as a numpy array, so we convert al of these as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from Pandas Dataframe to Numpy Arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply same Normalization for Test Features\n",
    "scal = StandardScaler()\n",
    "scal.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X_test to be the Scaled Version of Data\n",
    "# This process scales all the values in all columns and replaces them with the new values\n",
    "X_test = pd.DataFrame(data=scal.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "195 -0.435057  2.559632 -1.538110 -0.291730 -1.161905  1.923544 -1.382171   \n",
      "4   -0.429837 -0.461719 -1.285981 -0.291730 -0.871823  1.020070 -0.578775   \n",
      "434  0.878311 -0.461719  1.047677 -0.291730  1.182918 -0.145262  0.897735   \n",
      "458  0.296144 -0.461719  1.047677 -0.291730  1.182918 -0.029846  0.488800   \n",
      "39  -0.433751  2.370798 -1.173109 -0.291730 -1.113558  0.335019 -1.751298   \n",
      "304 -0.431151  0.784589 -1.285981 -0.291730 -0.759014  1.130523 -1.052851   \n",
      "225 -0.386572 -0.461719 -0.696703 -0.291730 -0.501164  2.978425  0.463467   \n",
      "32  -0.305211 -0.461719 -0.412325 -0.291730 -0.227199 -0.465449  0.427278   \n",
      "157 -0.320746 -0.461719  1.264625 -0.291730  0.312675  0.766899  0.984589   \n",
      "404  3.487718 -0.461719  1.047677 -0.291730  1.021762 -0.985443  0.550321   \n",
      "65  -0.432975  2.559632 -1.111543 -0.291730 -1.355292 -0.043497 -1.896054   \n",
      "138 -0.412758 -0.461719  1.603240 -0.291730  0.465773 -0.580866  1.013540   \n",
      "18  -0.360514 -0.461719 -0.412325 -0.291730 -0.227199 -1.078521 -1.215702   \n",
      "352 -0.429517  1.804295 -1.357808 -0.291730 -1.250540 -0.547358 -1.870722   \n",
      "114 -0.422915 -0.461719 -0.138208 -0.291730 -0.154678 -0.088174  0.506894   \n",
      "407  0.692894 -0.461719  1.047677 -0.291730  0.747796 -0.889883  1.078680   \n",
      "417  2.014757 -0.461719  1.047677 -0.291730  0.908953 -1.267158  0.684220   \n",
      "290 -0.433053  2.559632 -0.879936 -0.291730 -1.250540  0.665134 -1.530546   \n",
      "95  -0.424830 -0.461719 -1.181904 -0.291730 -0.976575  0.372250 -0.448495   \n",
      "321 -0.419203 -0.461719 -0.523731 -0.291730 -0.589800  0.063232 -0.575156   \n",
      "439  0.450956 -0.461719  1.047677 -0.291730  1.400479 -0.866304  0.857927   \n",
      "12  -0.427500  0.010367 -0.451903 -0.291730 -0.340008 -0.541152 -1.128848   \n",
      "505 -0.431882 -0.461719  0.143238 -0.291730  0.054825 -0.366166  0.383852   \n",
      "252 -0.428594  0.369153 -0.746542 -0.291730 -1.089384  0.784274 -2.294133   \n",
      "291 -0.428910  2.559632 -0.879936 -0.291730 -1.250540  1.021312 -1.537783   \n",
      "361 -0.073820 -0.461719  1.047677 -0.291730  1.642213 -0.091897  0.756598   \n",
      "234 -0.394039 -0.461719 -0.696703  3.427827 -0.476991  0.497594 -0.133651   \n",
      "128 -0.405612 -0.461719  1.603240 -0.291730  0.465773  0.131489  1.035253   \n",
      "372  0.344808 -0.461719  1.047677  3.427827  0.820317 -0.558527  0.702315   \n",
      "198 -0.432801  2.559632 -1.382728 -0.291730 -1.306945  1.177682 -1.154180   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "154 -0.302767 -0.461719  1.264625  3.427827  2.456052 -0.243304  0.933924   \n",
      "130 -0.404230 -0.461719  1.603240 -0.291730  0.465773  0.164997  1.038872   \n",
      "296 -0.431286 -0.461719  0.434945 -0.291730 -1.041037  0.277931 -0.694580   \n",
      "155 -0.102340 -0.461719  1.264625  3.427827  2.456052 -0.214760  0.448992   \n",
      "161 -0.298089 -0.461719  1.264625 -0.291730  0.312675  1.444505  0.745741   \n",
      "48  -0.412374 -0.461719 -0.592626 -0.291730 -0.952402 -1.149260  0.908592   \n",
      "308 -0.389780 -0.461719 -0.154333 -0.291730 -0.178852  0.384660  0.445373   \n",
      "401  0.908812 -0.461719  1.047677 -0.291730  1.021762  0.022278  1.078680   \n",
      "24  -0.365470 -0.461719 -0.412325 -0.291730 -0.227199 -0.497716  0.865165   \n",
      "462  0.192459 -0.461719  1.047677 -0.291730  1.182918 -0.009989  0.463467   \n",
      "175 -0.430065 -0.461719 -1.011864 -0.291730 -0.452817  0.274208 -1.342363   \n",
      "443  0.505373 -0.461719  1.047677 -0.291730  1.400479  0.198505  1.078680   \n",
      "8   -0.416402  0.010367 -0.451903 -0.291730 -0.340008 -0.861340  1.078680   \n",
      "330 -0.432068 -0.461719 -1.130599 -0.291730 -0.855708 -0.224688 -1.374933   \n",
      "479  0.918025 -0.461719  1.047677 -0.291730  0.385195 -0.119200  0.644412   \n",
      "187 -0.428921  1.237791 -1.101282 -0.291730 -1.041037  0.567092 -1.052851   \n",
      "38  -0.419821 -0.461719 -0.731884 -0.291730 -0.541453 -0.445593 -1.447311   \n",
      "432  0.172534 -0.461719  1.047677 -0.291730  0.143461  0.124043  0.166718   \n",
      "382  0.431717 -0.461719  1.047677 -0.291730  1.078167 -0.979238  1.078680   \n",
      "143 -0.049200 -0.461719  1.264625 -0.291730  2.456052 -1.063628  1.078680   \n",
      "360 -0.007197 -0.461719  1.047677 -0.291730  1.642213  0.090535  0.644412   \n",
      "263 -0.358383  0.293619 -1.023591 -0.291730  0.651103  1.243457  0.879641   \n",
      "398  3.187487 -0.461719  1.047677 -0.291730  1.021762 -1.082244  1.078680   \n",
      "186 -0.431068 -0.461719 -1.244937 -0.291730 -0.630089  1.868939 -0.600489   \n",
      "147 -0.212552 -0.461719  1.264625 -0.291730  2.456052 -1.736270  0.923067   \n",
      "227 -0.397396 -0.461719 -0.696703 -0.291730 -0.501164  1.039927  0.351282   \n",
      "405  5.981451 -0.461719  1.047677 -0.291730  1.021762 -0.796806  1.078680   \n",
      "69  -0.424252  0.010367 -0.715759 -0.291730 -1.266656 -0.546117 -1.345982   \n",
      "231 -0.392617 -0.461719 -0.696703 -0.291730 -0.501164  1.348945  0.242715   \n",
      "104 -0.423171 -0.461719 -0.350759 -0.291730 -0.372239 -0.196144  0.716790   \n",
      "\n",
      "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "195  0.913879 -0.651309 -0.959881 -1.866931  0.440393 -1.189960  \n",
      "4    1.103706 -0.764100 -1.158746  0.068394  0.470351 -0.902458  \n",
      "434 -0.657862  1.604499  1.516892  0.743507 -2.853820  0.296281  \n",
      "458 -0.400554  1.604499  1.516892  0.743507 -0.928680  0.425413  \n",
      "39   0.800432 -0.764100 -0.977959 -0.111636  0.456102 -1.025499  \n",
      "304  0.167781 -0.312938 -1.158746 -0.066629  0.434222 -0.707541  \n",
      "225 -0.349496 -0.200148 -0.646518 -0.516704  0.303172 -0.987734  \n",
      "32   0.153102 -0.651309 -0.646518  1.103568 -1.373107  1.823941  \n",
      "157 -0.816083 -0.538519 -0.068002 -1.731908  0.094815 -0.992607  \n",
      "404 -0.939897  1.604499  1.516892  0.743507 -0.286331  1.783739  \n",
      "65   1.355693 -0.651309 -0.465732 -1.101803  0.470351 -0.982861  \n",
      "138 -0.911822 -0.651309  0.136889  1.193583  0.415821  1.045492  \n",
      "18   0.064335 -0.651309 -0.646518  1.103568 -0.740407 -0.127663  \n",
      "352  3.235985 -0.651309 -0.019792 -0.111636  0.419075 -0.602773  \n",
      "114 -0.642128 -0.425728  0.106758 -0.336674  0.378795 -0.278724  \n",
      "407 -1.087704  1.604499  1.516892  0.743507 -0.256822 -0.074061  \n",
      "417 -0.921501  1.604499  1.516892  0.743507 -2.553908  1.693590  \n",
      "290  0.669966 -0.651309 -1.020143  0.293432  0.470351 -1.146104  \n",
      "95  -0.073884 -0.876890 -0.833330 -0.246659  0.033666 -0.741651  \n",
      "321  0.405593 -0.538519 -0.767042  0.473462  0.470351 -0.714850  \n",
      "439 -0.843653  1.604499  1.516892  0.743507  0.470351  1.235536  \n",
      "12   0.823278 -0.538519 -0.622413 -1.506871  0.398543  0.362065  \n",
      "505 -0.528130 -0.989680 -0.851409  1.103568  0.470351 -0.591809  \n",
      "252  2.408598 -0.312938 -0.507915  0.248424  0.349062 -1.121739  \n",
      "291  0.669966 -0.651309 -1.020143  0.293432  0.470351 -1.118084  \n",
      "361 -0.624237  1.604499  1.516892  0.743507 -0.048577  0.176894  \n",
      "234 -0.001999 -0.200148 -0.646518 -0.516704  0.058574 -0.571099  \n",
      "128 -0.845809 -0.651309  0.136889  1.193583  0.470351  0.323082  \n",
      "372 -1.159084  1.604499  1.516892  0.743507 -0.079657 -0.469986  \n",
      "198  1.675666 -0.876890 -0.513941 -2.677067  0.417617 -0.745306  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "154 -0.874756 -0.538519 -0.068002 -1.731908 -0.381028  0.290190  \n",
      "130 -0.705434 -0.651309  0.136889  1.193583  0.449482 -0.016804  \n",
      "296  1.057007 -0.651309 -0.754990 -1.146810  0.424910 -0.651502  \n",
      "155 -0.876545 -0.538519 -0.068002 -1.731908 -2.995418  0.278007  \n",
      "161 -0.773144 -0.538519 -0.068002 -1.731908  0.218236 -1.341020  \n",
      "48   1.015536 -0.764100 -1.092457 -0.291667  0.470351  2.201592  \n",
      "308 -0.155402 -0.651309 -0.664596 -0.066629  0.470351 -0.998698  \n",
      "401 -0.955173  1.604499  1.516892  0.743507  0.470351  0.923669  \n",
      "24   0.341002 -0.651309 -0.646518  1.103568  0.441515  0.433941  \n",
      "462 -0.422895  1.604499  1.516892  0.743507  0.470351  0.152530  \n",
      "175 -0.240361 -0.538519 -0.712806 -0.876765  0.403704 -0.902458  \n",
      "443 -0.769704  1.604499  1.516892  0.743507  0.356243  0.744589  \n",
      "8    1.112835 -0.538519 -0.622413 -1.506871  0.355121  2.094388  \n",
      "330  1.017188 -0.651309  0.094706 -0.741742  0.152486 -0.444403  \n",
      "479 -0.782181  1.604499  1.516892  0.743507  0.317982  0.045325  \n",
      "187  0.060711 -0.538519 -0.098133 -1.506871  0.436354 -0.737996  \n",
      "38   0.087639 -0.538519 -0.815252  0.293432  0.431417 -0.317707  \n",
      "432 -0.667863  1.604499  1.516892  0.743507 -2.883890 -0.086244  \n",
      "382 -0.952283  1.604499  1.516892  0.743507  0.470351  1.323248  \n",
      "143 -1.029627 -0.538519 -0.068002 -1.731908  0.470351  1.666789  \n",
      "360 -0.522075  1.604499  1.516892  0.743507  0.219695 -0.602773  \n",
      "263 -0.723646 -0.538519 -0.905645 -2.497037  0.431305 -0.181265  \n",
      "398 -0.993937  1.604499  1.516892  0.743507  0.470351  2.174791  \n",
      "186 -0.209672 -0.764100 -1.333506 -0.336674  0.422441 -1.009662  \n",
      "147 -1.007149 -0.538519 -0.068002 -1.731908  0.412119  2.045659  \n",
      "227 -0.202102 -0.200148 -0.646518 -0.516704  0.191869 -0.776980  \n",
      "405 -1.023388  1.604499  1.516892  0.743507  0.336496  1.247718  \n",
      "69   1.303626 -0.651309 -0.417522  0.158409  0.470351 -0.480950  \n",
      "231  0.006992 -0.200148 -0.646518 -0.516704  0.237422 -0.912203  \n",
      "104 -0.566665 -0.538519 -0.182500  1.058560  0.423114 -0.049697  \n",
      "\n",
      "[102 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test features and Labels to Numpy Arrays\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test), type(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Columns for Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Feature Columns\n",
    "feat_cols = [tf.feature_column.numeric_column('x', shape=np.array(X_train).shape[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Input Function\n",
    "input_func = tf.estimator.inputs.numpy_input_fn({'x':X_train}, y_train, batch_size=1, num_epochs=2000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\n",
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_is_chief': True, '_model_dir': 'C:\\\\Users\\\\HAREKR~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfgao2mfz', '_task_type': 'worker', '_save_summary_steps': 100, '_master': '', '_service': None, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_log_step_count_steps': 100, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027C328305F8>, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "# Define Linear Regressor Model\n",
    "linear_model = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Estimator Training Inputs\n",
    "train_input_func = tf.estimator.inputs.numpy_input_fn(X_train, y_train, batch_size=1, num_epochs=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Estimator Test Inputs\n",
    "eval_input_func = tf.estimator.inputs.numpy_input_fn({'x': X_test}, y_test, batch_size=1, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 484.0\n",
      "INFO:tensorflow:global_step/sec: 628.482\n",
      "INFO:tensorflow:step = 101, loss = 838.98065 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.404\n",
      "INFO:tensorflow:step = 201, loss = 3.539871 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 805.878\n",
      "INFO:tensorflow:step = 301, loss = 2.4805553 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.194\n",
      "INFO:tensorflow:step = 401, loss = 0.13523738 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 740.217\n",
      "INFO:tensorflow:step = 501, loss = 0.09162616 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 703.72\n",
      "INFO:tensorflow:step = 601, loss = 1.9461008 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 698.803\n",
      "INFO:tensorflow:step = 701, loss = 0.1882087 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 713.776\n",
      "INFO:tensorflow:step = 801, loss = 6.058448 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 757.032\n",
      "INFO:tensorflow:step = 901, loss = 7.2218156 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 762.821\n",
      "INFO:tensorflow:step = 1001, loss = 40.567265 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.396\n",
      "INFO:tensorflow:step = 1101, loss = 1.295501 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.423\n",
      "INFO:tensorflow:step = 1201, loss = 18.387304 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 693.94\n",
      "INFO:tensorflow:step = 1301, loss = 70.29969 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.898\n",
      "INFO:tensorflow:step = 1401, loss = 27.856256 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 751.335\n",
      "INFO:tensorflow:step = 1501, loss = 54.707077 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.412\n",
      "INFO:tensorflow:step = 1601, loss = 16.555521 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.405\n",
      "INFO:tensorflow:step = 1701, loss = 89.570015 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.415\n",
      "INFO:tensorflow:step = 1801, loss = 9.673658 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 762.792\n",
      "INFO:tensorflow:step = 1901, loss = 2.6420324 (0.130 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.8769511.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearRegressor at 0x27c327f86a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Linear Regressor Estimator\n",
    "linear_model.train(input_fn=input_func, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-24-01:16:15\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-24-01:16:16\n",
      "INFO:tensorflow:Saving dict for global step 2000: average_loss = 36.82935, global_step = 2000, loss = 36.82935\n"
     ]
    }
   ],
   "source": [
    "test_metrics = linear_model.evaluate(input_fn=eval_input_func, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'predictions': array([36.96878], dtype=float32)},\n",
       " {'predictions': array([28.247849], dtype=float32)},\n",
       " {'predictions': array([13.189707], dtype=float32)},\n",
       " {'predictions': array([15.252672], dtype=float32)},\n",
       " {'predictions': array([30.13174], dtype=float32)},\n",
       " {'predictions': array([31.973793], dtype=float32)},\n",
       " {'predictions': array([36.310623], dtype=float32)},\n",
       " {'predictions': array([10.222073], dtype=float32)},\n",
       " {'predictions': array([30.608047], dtype=float32)},\n",
       " {'predictions': array([3.2595158], dtype=float32)},\n",
       " {'predictions': array([28.537374], dtype=float32)},\n",
       " {'predictions': array([14.321341], dtype=float32)},\n",
       " {'predictions': array([18.192257], dtype=float32)},\n",
       " {'predictions': array([17.608517], dtype=float32)},\n",
       " {'predictions': array([24.481892], dtype=float32)},\n",
       " {'predictions': array([17.87411], dtype=float32)},\n",
       " {'predictions': array([3.9541264], dtype=float32)},\n",
       " {'predictions': array([31.663893], dtype=float32)},\n",
       " {'predictions': array([28.317438], dtype=float32)},\n",
       " {'predictions': array([25.531713], dtype=float32)},\n",
       " {'predictions': array([12.029847], dtype=float32)},\n",
       " {'predictions': array([21.143948], dtype=float32)},\n",
       " {'predictions': array([23.837793], dtype=float32)},\n",
       " {'predictions': array([24.78284], dtype=float32)},\n",
       " {'predictions': array([32.260883], dtype=float32)},\n",
       " {'predictions': array([17.794544], dtype=float32)},\n",
       " {'predictions': array([30.197895], dtype=float32)},\n",
       " {'predictions': array([18.725657], dtype=float32)},\n",
       " {'predictions': array([23.103472], dtype=float32)},\n",
       " {'predictions': array([31.199345], dtype=float32)},\n",
       " {'predictions': array([20.769033], dtype=float32)},\n",
       " {'predictions': array([20.28313], dtype=float32)},\n",
       " {'predictions': array([34.240402], dtype=float32)},\n",
       " {'predictions': array([39.790592], dtype=float32)},\n",
       " {'predictions': array([29.652424], dtype=float32)},\n",
       " {'predictions': array([23.393143], dtype=float32)},\n",
       " {'predictions': array([13.395846], dtype=float32)},\n",
       " {'predictions': array([19.563663], dtype=float32)},\n",
       " {'predictions': array([2.570692], dtype=float32)},\n",
       " {'predictions': array([31.2663], dtype=float32)},\n",
       " {'predictions': array([24.725657], dtype=float32)},\n",
       " {'predictions': array([18.077593], dtype=float32)},\n",
       " {'predictions': array([33.30214], dtype=float32)},\n",
       " {'predictions': array([13.263567], dtype=float32)},\n",
       " {'predictions': array([16.753414], dtype=float32)},\n",
       " {'predictions': array([25.533527], dtype=float32)},\n",
       " {'predictions': array([30.246292], dtype=float32)},\n",
       " {'predictions': array([16.548645], dtype=float32)},\n",
       " {'predictions': array([26.697819], dtype=float32)},\n",
       " {'predictions': array([24.028648], dtype=float32)},\n",
       " {'predictions': array([31.345562], dtype=float32)},\n",
       " {'predictions': array([31.842293], dtype=float32)},\n",
       " {'predictions': array([21.35001], dtype=float32)},\n",
       " {'predictions': array([8.652537], dtype=float32)},\n",
       " {'predictions': array([30.016901], dtype=float32)},\n",
       " {'predictions': array([0.24879265], dtype=float32)},\n",
       " {'predictions': array([20.124712], dtype=float32)},\n",
       " {'predictions': array([18.038857], dtype=float32)},\n",
       " {'predictions': array([22.729448], dtype=float32)},\n",
       " {'predictions': array([18.752705], dtype=float32)},\n",
       " {'predictions': array([27.868996], dtype=float32)},\n",
       " {'predictions': array([5.4575768], dtype=float32)},\n",
       " {'predictions': array([15.939549], dtype=float32)},\n",
       " {'predictions': array([20.118494], dtype=float32)},\n",
       " {'predictions': array([6.942293], dtype=float32)},\n",
       " {'predictions': array([24.394865], dtype=float32)},\n",
       " {'predictions': array([24.84462], dtype=float32)},\n",
       " {'predictions': array([20.484081], dtype=float32)},\n",
       " {'predictions': array([15.135431], dtype=float32)},\n",
       " {'predictions': array([21.045486], dtype=float32)},\n",
       " {'predictions': array([23.443481], dtype=float32)},\n",
       " {'predictions': array([23.538694], dtype=float32)},\n",
       " {'predictions': array([21.814835], dtype=float32)},\n",
       " {'predictions': array([19.856955], dtype=float32)},\n",
       " {'predictions': array([26.485432], dtype=float32)},\n",
       " {'predictions': array([18.93871], dtype=float32)},\n",
       " {'predictions': array([33.42759], dtype=float32)},\n",
       " {'predictions': array([11.117362], dtype=float32)},\n",
       " {'predictions': array([28.492971], dtype=float32)},\n",
       " {'predictions': array([15.03846], dtype=float32)},\n",
       " {'predictions': array([17.52425], dtype=float32)},\n",
       " {'predictions': array([18.096256], dtype=float32)},\n",
       " {'predictions': array([30.23843], dtype=float32)},\n",
       " {'predictions': array([15.89113], dtype=float32)},\n",
       " {'predictions': array([12.380888], dtype=float32)},\n",
       " {'predictions': array([21.633806], dtype=float32)},\n",
       " {'predictions': array([18.698055], dtype=float32)},\n",
       " {'predictions': array([30.645996], dtype=float32)},\n",
       " {'predictions': array([23.950607], dtype=float32)},\n",
       " {'predictions': array([18.13381], dtype=float32)},\n",
       " {'predictions': array([12.1811], dtype=float32)},\n",
       " {'predictions': array([13.083127], dtype=float32)},\n",
       " {'predictions': array([21.35173], dtype=float32)},\n",
       " {'predictions': array([32.462578], dtype=float32)},\n",
       " {'predictions': array([2.9101696], dtype=float32)},\n",
       " {'predictions': array([34.578526], dtype=float32)},\n",
       " {'predictions': array([10.304222], dtype=float32)},\n",
       " {'predictions': array([30.986065], dtype=float32)},\n",
       " {'predictions': array([1.7123241], dtype=float32)},\n",
       " {'predictions': array([21.462265], dtype=float32)},\n",
       " {'predictions': array([31.67812], dtype=float32)},\n",
       " {'predictions': array([21.888216], dtype=float32)}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted Values\n",
    "list(linear_model.predict(input_fn=eval_input_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = linear_model.predict(input_fn=eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "pred = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "# Plot Predicted Values\n",
    "predicted_vals = []\n",
    "\n",
    "for pred in linear_model.predict(input_fn=eval_input_func):\n",
    "    predicted_vals.append(pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([36.96878], dtype=float32), array([28.247849], dtype=float32), array([13.189707], dtype=float32), array([15.252672], dtype=float32), array([30.13174], dtype=float32), array([31.973793], dtype=float32), array([36.310623], dtype=float32), array([10.222073], dtype=float32), array([30.608047], dtype=float32), array([3.2595158], dtype=float32), array([28.537374], dtype=float32), array([14.321341], dtype=float32), array([18.192257], dtype=float32), array([17.608517], dtype=float32), array([24.481892], dtype=float32), array([17.87411], dtype=float32), array([3.9541264], dtype=float32), array([31.663893], dtype=float32), array([28.317438], dtype=float32), array([25.531713], dtype=float32), array([12.029847], dtype=float32), array([21.143948], dtype=float32), array([23.837793], dtype=float32), array([24.78284], dtype=float32), array([32.260883], dtype=float32), array([17.794544], dtype=float32), array([30.197895], dtype=float32), array([18.725657], dtype=float32), array([23.103472], dtype=float32), array([31.199345], dtype=float32), array([20.769033], dtype=float32), array([20.28313], dtype=float32), array([34.240402], dtype=float32), array([39.790592], dtype=float32), array([29.652424], dtype=float32), array([23.393143], dtype=float32), array([13.395846], dtype=float32), array([19.563663], dtype=float32), array([2.570692], dtype=float32), array([31.2663], dtype=float32), array([24.725657], dtype=float32), array([18.077593], dtype=float32), array([33.30214], dtype=float32), array([13.263567], dtype=float32), array([16.753414], dtype=float32), array([25.533527], dtype=float32), array([30.246292], dtype=float32), array([16.548645], dtype=float32), array([26.697819], dtype=float32), array([24.028648], dtype=float32), array([31.345562], dtype=float32), array([31.842293], dtype=float32), array([21.35001], dtype=float32), array([8.652537], dtype=float32), array([30.016901], dtype=float32), array([0.24879265], dtype=float32), array([20.124712], dtype=float32), array([18.038857], dtype=float32), array([22.729448], dtype=float32), array([18.752705], dtype=float32), array([27.868996], dtype=float32), array([5.4575768], dtype=float32), array([15.939549], dtype=float32), array([20.118494], dtype=float32), array([6.942293], dtype=float32), array([24.394865], dtype=float32), array([24.84462], dtype=float32), array([20.484081], dtype=float32), array([15.135431], dtype=float32), array([21.045486], dtype=float32), array([23.443481], dtype=float32), array([23.538694], dtype=float32), array([21.814835], dtype=float32), array([19.856955], dtype=float32), array([26.485432], dtype=float32), array([18.93871], dtype=float32), array([33.42759], dtype=float32), array([11.117362], dtype=float32), array([28.492971], dtype=float32), array([15.03846], dtype=float32), array([17.52425], dtype=float32), array([18.096256], dtype=float32), array([30.23843], dtype=float32), array([15.89113], dtype=float32), array([12.380888], dtype=float32), array([21.633806], dtype=float32), array([18.698055], dtype=float32), array([30.645996], dtype=float32), array([23.950607], dtype=float32), array([18.13381], dtype=float32), array([12.1811], dtype=float32), array([13.083127], dtype=float32), array([21.35173], dtype=float32), array([32.462578], dtype=float32), array([2.9101696], dtype=float32), array([34.578526], dtype=float32), array([10.304222], dtype=float32), array([30.986065], dtype=float32), array([1.7123241], dtype=float32), array([21.462265], dtype=float32), array([31.67812], dtype=float32), array([21.888216], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claculate Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Mean Squared Error from Scikit Learn\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(predicted_vals, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error [LinearRegrssor]:  36.13857046693569\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error [LinearRegrssor]: ',mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Regressor using TF Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpprpbvw30\n",
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_is_chief': True, '_model_dir': 'C:\\\\Users\\\\HAREKR~1\\\\AppData\\\\Local\\\\Temp\\\\tmpprpbvw30', '_task_type': 'worker', '_save_summary_steps': 100, '_master': '', '_service': None, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_log_step_count_steps': 100, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027C32F71EF0>, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "# Define DNN Regressor Model\n",
    "dnn_model = tf.estimator.DNNRegressor(hidden_units=[10,10,10],feature_columns=feat_cols, optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpprpbvw30\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 78.373085\n",
      "INFO:tensorflow:global_step/sec: 357.373\n",
      "INFO:tensorflow:step = 101, loss = 1.8842732 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.058\n",
      "INFO:tensorflow:step = 201, loss = 1.654692 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 644.7\n",
      "INFO:tensorflow:step = 301, loss = 1.7087057 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.197\n",
      "INFO:tensorflow:step = 401, loss = 7.248042 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.569\n",
      "INFO:tensorflow:step = 501, loss = 38.50322 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 713.778\n",
      "INFO:tensorflow:step = 601, loss = 1.2973578 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 657.427\n",
      "INFO:tensorflow:step = 701, loss = 19.445608 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.133\n",
      "INFO:tensorflow:step = 801, loss = 0.17938946 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.87\n",
      "INFO:tensorflow:step = 901, loss = 5.8082924 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.091\n",
      "INFO:tensorflow:step = 1001, loss = 3.4418159 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 670.669\n",
      "INFO:tensorflow:step = 1101, loss = 6.305474 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.554\n",
      "INFO:tensorflow:step = 1201, loss = 0.50430995 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.195\n",
      "INFO:tensorflow:step = 1301, loss = 0.2226896 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.061\n",
      "INFO:tensorflow:step = 1401, loss = 0.0007991259 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.814\n",
      "INFO:tensorflow:step = 1501, loss = 0.20668662 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.461\n",
      "INFO:tensorflow:step = 1601, loss = 0.05123554 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 693.947\n",
      "INFO:tensorflow:step = 1701, loss = 8.258454 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.556\n",
      "INFO:tensorflow:step = 1801, loss = 0.021896308 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.192\n",
      "INFO:tensorflow:step = 1901, loss = 10.085296 (0.151 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpprpbvw30\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.217078.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNRegressor at 0x27c32f710b8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the DNN Regressor Estimator\n",
    "dnn_model.train(input_fn=input_func, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-24-01:16:33\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpprpbvw30\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-24-01:16:33\n",
      "INFO:tensorflow:Saving dict for global step 2000: average_loss = 41.148724, global_step = 2000, loss = 41.148724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'average_loss': 41.148724, 'global_step': 2000, 'loss': 41.148724}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(input_fn=eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = dnn_model.predict(input_fn=eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpprpbvw30\\model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "pred = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HAREKR~1\\AppData\\Local\\Temp\\tmpfgao2mfz\\model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "# Plot Predicted Values\n",
    "predicted_vals = []\n",
    "\n",
    "for pred in linear_model.predict(input_fn=eval_input_func):\n",
    "    predicted_vals.append(pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([36.96878], dtype=float32), array([28.247849], dtype=float32), array([13.189707], dtype=float32), array([15.252672], dtype=float32), array([30.13174], dtype=float32), array([31.973793], dtype=float32), array([36.310623], dtype=float32), array([10.222073], dtype=float32), array([30.608047], dtype=float32), array([3.2595158], dtype=float32), array([28.537374], dtype=float32), array([14.321341], dtype=float32), array([18.192257], dtype=float32), array([17.608517], dtype=float32), array([24.481892], dtype=float32), array([17.87411], dtype=float32), array([3.9541264], dtype=float32), array([31.663893], dtype=float32), array([28.317438], dtype=float32), array([25.531713], dtype=float32), array([12.029847], dtype=float32), array([21.143948], dtype=float32), array([23.837793], dtype=float32), array([24.78284], dtype=float32), array([32.260883], dtype=float32), array([17.794544], dtype=float32), array([30.197895], dtype=float32), array([18.725657], dtype=float32), array([23.103472], dtype=float32), array([31.199345], dtype=float32), array([20.769033], dtype=float32), array([20.28313], dtype=float32), array([34.240402], dtype=float32), array([39.790592], dtype=float32), array([29.652424], dtype=float32), array([23.393143], dtype=float32), array([13.395846], dtype=float32), array([19.563663], dtype=float32), array([2.570692], dtype=float32), array([31.2663], dtype=float32), array([24.725657], dtype=float32), array([18.077593], dtype=float32), array([33.30214], dtype=float32), array([13.263567], dtype=float32), array([16.753414], dtype=float32), array([25.533527], dtype=float32), array([30.246292], dtype=float32), array([16.548645], dtype=float32), array([26.697819], dtype=float32), array([24.028648], dtype=float32), array([31.345562], dtype=float32), array([31.842293], dtype=float32), array([21.35001], dtype=float32), array([8.652537], dtype=float32), array([30.016901], dtype=float32), array([0.24879265], dtype=float32), array([20.124712], dtype=float32), array([18.038857], dtype=float32), array([22.729448], dtype=float32), array([18.752705], dtype=float32), array([27.868996], dtype=float32), array([5.4575768], dtype=float32), array([15.939549], dtype=float32), array([20.118494], dtype=float32), array([6.942293], dtype=float32), array([24.394865], dtype=float32), array([24.84462], dtype=float32), array([20.484081], dtype=float32), array([15.135431], dtype=float32), array([21.045486], dtype=float32), array([23.443481], dtype=float32), array([23.538694], dtype=float32), array([21.814835], dtype=float32), array([19.856955], dtype=float32), array([26.485432], dtype=float32), array([18.93871], dtype=float32), array([33.42759], dtype=float32), array([11.117362], dtype=float32), array([28.492971], dtype=float32), array([15.03846], dtype=float32), array([17.52425], dtype=float32), array([18.096256], dtype=float32), array([30.23843], dtype=float32), array([15.89113], dtype=float32), array([12.380888], dtype=float32), array([21.633806], dtype=float32), array([18.698055], dtype=float32), array([30.645996], dtype=float32), array([23.950607], dtype=float32), array([18.13381], dtype=float32), array([12.1811], dtype=float32), array([13.083127], dtype=float32), array([21.35173], dtype=float32), array([32.462578], dtype=float32), array([2.9101696], dtype=float32), array([34.578526], dtype=float32), array([10.304222], dtype=float32), array([30.986065], dtype=float32), array([1.7123241], dtype=float32), array([21.462265], dtype=float32), array([31.67812], dtype=float32), array([21.888216], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(predicted_vals, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error [DNNRegrssor]:  36.13857046693569\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error [DNNRegrssor]: ',mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = tf.placeholder(tf.float32,[None,X_train.shape[1]])\n",
    "\n",
    "# Labels\n",
    "y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "# Weights\n",
    "W = tf.Variable(tf.ones([13,1]))\n",
    "\n",
    "b = tf.Variable(tf.ones(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_hat = tf.add(tf.matmul(X, W), b)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.square(y - y_hat))\n",
    "\n",
    "#\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error: 585.47216796875\n",
      "Epoch: 100, Error: 39.73214340209961\n",
      "Epoch: 200, Error: 20.127126693725586\n",
      "Epoch: 300, Error: 19.244400024414062\n",
      "Epoch: 400, Error: 19.204639434814453\n",
      "Epoch: 500, Error: 19.20282554626465\n",
      "Epoch: 600, Error: 19.202743530273438\n",
      "Epoch: 700, Error: 19.202777862548828\n",
      "Epoch: 800, Error: 19.202783584594727\n",
      "Epoch: 900, Error: 19.202770233154297\n",
      "Epoch: 1000, Error: 19.202774047851562\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(0,epochs):\n",
    "        result, err = sess.run([optimizer, cost], feed_dict={X: X_train, y: y_train})\n",
    "        cost_history = np.append(cost_history,sess.run(cost,feed_dict={X: X_train, y: y_train}))\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print('Epoch: {0}, Error: {1}'.format(epoch, err))\n",
    "    \n",
    "    print('Epoch: {0}, Error: {1}'.format(epoch+1, err))\n",
    "    \n",
    "    # Values of Weight & Bias after Training\n",
    "    new_W = sess.run(W)\n",
    "    new_b = sess.run(b)\n",
    "    \n",
    "    # Predicted Labels\n",
    "    y_pred = sess.run(y_hat, feed_dict={X: X_test})\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    mse = sess.run(tf.reduce_mean(tf.square(y_pred - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.7803233 ],\n",
       "        [ 0.95225394],\n",
       "        [ 0.11739684],\n",
       "        [ 0.76530427],\n",
       "        [-1.7784488 ],\n",
       "        [ 2.6996262 ],\n",
       "        [-0.02095263],\n",
       "        [-3.0123353 ],\n",
       "        [ 2.3134327 ],\n",
       "        [-1.7682383 ],\n",
       "        [-1.9098078 ],\n",
       "        [ 0.7654347 ],\n",
       "        [-3.7617311 ]], dtype=float32),\n",
       " array([22.33682, 22.33682, 22.33682, 22.33682, 22.33682, 22.33682,\n",
       "        22.33682, 22.33682, 22.33682, 22.33682, 22.33682, 22.33682,\n",
       "        22.33682], dtype=float32))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_W, new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values: \n",
      " [[37.8146   37.8146   37.8146   ... 37.8146   37.8146   37.8146  ]\n",
      " [26.755474 26.755474 26.755474 ... 26.755474 26.755474 26.755474]\n",
      " [16.889269 16.889269 16.889269 ... 16.889269 16.889269 16.889269]\n",
      " ...\n",
      " [20.348286 20.348286 20.348286 ... 20.348286 20.348286 20.348286]\n",
      " [31.685425 31.685425 31.685425 ... 31.685425 31.685425 31.685425]\n",
      " [21.353415 21.353415 21.353415 ... 21.353415 21.353415 21.353415]]\n"
     ]
    }
   ],
   "source": [
    "# Predicted Values\n",
    "print('Predicted Values: \\n',y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error [TF Session]:  34.86722743962253\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error\n",
    "print('Mean Squared Error [TF Session]: ',mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cost vs Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c32edaef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,epochs,0,np.max(cost_history)])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Epochs', fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
